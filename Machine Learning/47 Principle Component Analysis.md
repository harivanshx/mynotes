[[Machine Learning]]

![[Pasted image 20251217003333.png]]


  Feature Extraction with pca   
   Principle Component Analysis   


Photos are 2d but moment is in 3d so same pca does take the multiple feature to lower dimension

pca while keeping the essence of data -

benifits - higher to lower so lower size - faster execution of algo

visualization is easy on less dimention

Why variance is important 

so we will able to get the difference of spread 

spread is directly proportional to variance




![[Pasted image 20251217010021.png]]

Linear Transformation , Eigen Vectors and Eigen value


Eigan vactors are the vactors when you visualize them in linear transformation there direction remains same but scale can be differ

What are eigen values

Eigen values are the value of  how much coreespond to the vector our new magnitude change 
as you can see here ther is 3 time change so eigen value is three
![[Pasted image 20251217010647.png]]




![[Pasted image 20251217010740.png]]

covariance matrix eigen value which is max on that we do have the maximum variance


### Step By Step Solution 


- Mean 
- FInd cov matrix 
- Find the eigen value/ vector for matrix
- Find the max of eigen vector
- Now we have principle components 


3d to 1d


